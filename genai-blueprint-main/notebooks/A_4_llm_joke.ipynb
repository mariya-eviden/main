{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Joke (First LLM calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from devtools import debug\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(verbose=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!export PYTHONPATH=\":./python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Factory\n",
    "\n",
    "To facilitate the selection and configuration of LLM and their provider, we have an \"LLM Factory'. <br> <br>\n",
    "See [llm.py](../python/ai_core/llm.py)  <br>\n",
    "\n",
    "List of hard-coded LLM configuration is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-12 16:54:45.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpython.config\u001b[0m:\u001b[36myaml_file_config\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mload /mnt/c/Users/a884470/prj/genai-blueprint-main/app_conf.yaml\u001b[0m\n",
      "\u001b[32m2024-11-12 16:54:45.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpython.config\u001b[0m:\u001b[36myaml_file_config\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mOverride config from env. variable: azure\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['llama32_3_ollama',\n",
       " 'gemma2_2_ollama',\n",
       " 'llava_phi3_ollama',\n",
       " 'gpt_4o_edenai',\n",
       " 'gpt_4_edenai',\n",
       " 'gpt_4omini_edenai',\n",
       " 'mistral_large_edenai',\n",
       " 'google_gemini15flash_edenai',\n",
       " 'gpt_4_azure',\n",
       " 'gpt_35_azure',\n",
       " 'gpt_4o_azure']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from python.ai_core.llm import LlmFactory, get_llm\n",
    "\n",
    "LlmFactory().known_items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of an LLM and LLM provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-12 16:54:55.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpython.ai_core.llm\u001b[0m:\u001b[36mget_llm\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1mget LLM:'gpt_4_azure'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8319/3821042422.py:5 <module>\n",
      "    llm_default: AzureChatOpenAI(\n",
      "        name='gpt4-turbo',\n",
      "        client=<openai.resources.chat.completions.Completions object at 0x7ff6da34fef0>,\n",
      "        async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ff6da1d3440>,\n",
      "        root_client=<openai.lib.azure.AzureOpenAI object at 0x7ff6da6d25a0>,\n",
      "        root_async_client=<openai.lib.azure.AsyncAzureOpenAI object at 0x7ff6da34ff20>,\n",
      "        model_name='gpt4-turbo',\n",
      "        temperature=0.0,\n",
      "        model_kwargs={},\n",
      "        openai_api_key=SecretStr('**********'),\n",
      "        disabled_params={\n",
      "            'parallel_tool_calls': None,\n",
      "        },\n",
      "        azure_endpoint='https://mutualizedopenai.openai.azure.com',\n",
      "        deployment_name='gpt4-turbo',\n",
      "        openai_api_version='2023-05-15',\n",
      "        openai_api_type='azure',\n",
      "    ) (AzureChatOpenAI)\n"
     ]
    }
   ],
   "source": [
    "# Get the default LLM. We can configure the temperature, cache, max_token, ...\n",
    "\n",
    "\n",
    "llm_default = get_llm()\n",
    "debug(llm_default)\n",
    "\n",
    "# Note that the LLM is configurable, ie we can change the LLM at run time\n",
    "# See https://python.langchain.com/v0.1/docs/expression_language/primitives/configure/#with-llms-1\n",
    "# (not necessary for the training)\n",
    "\n",
    "# or get a given LLM;\n",
    "gpt4 = LlmFactory(\n",
    "    llm_id=\"gpt_4o_azure\"\n",
    ").get()  # Might NOT work if you din't have the API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a prompt to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8319/159402040.py:2 <module>\n",
      "    a: AIMessage(\n",
      "        content=(\n",
      "            \"Why don't skeletons fight each other?\\n\"\n",
      "            '\\n'\n",
      "            \"Because they don't have the guts.\"\n",
      "        ),\n",
      "        additional_kwargs={\n",
      "            'refusal': None,\n",
      "        },\n",
      "        response_metadata={\n",
      "            'token_usage': {\n",
      "                'completion_tokens': 16,\n",
      "                'prompt_tokens': 12,\n",
      "                'total_tokens': 28,\n",
      "                'completion_tokens_details': None,\n",
      "                'prompt_tokens_details': None,\n",
      "            },\n",
      "            'model_name': 'gpt-4',\n",
      "            'system_fingerprint': 'fp_5603ee5e2e',\n",
      "            'finish_reason': 'stop',\n",
      "            'logprobs': None,\n",
      "            'content_filter_results': {},\n",
      "        },\n",
      "        id='run-3bd8ebb0-d105-4462-95ce-1049220f0e08-0',\n",
      "        usage_metadata={\n",
      "            'input_tokens': 12,\n",
      "            'output_tokens': 16,\n",
      "            'total_tokens': 28,\n",
      "            'input_token_details': {},\n",
      "            'output_token_details': {},\n",
      "        },\n",
      "    ) (AIMessage)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why don't skeletons fight each other?\\n\\nBecause they don't have the guts.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 12, 'total_tokens': 28, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4', 'system_fingerprint': 'fp_5603ee5e2e', 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-3bd8ebb0-d105-4462-95ce-1049220f0e08-0', usage_metadata={'input_tokens': 12, 'output_tokens': 16, 'total_tokens': 28, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = llm_default.invoke(\"tell me a sad joke\")\n",
    "debug(a)\n",
    "# Analyse the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call LLM ; Change dynamically.\n",
    "\n",
    "To be updated with available LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-12 17:00:54.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpython.ai_core.llm\u001b[0m:\u001b[36mget_llm\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1mget LLM:'gpt_4_azure'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8319/2064223036.py:11 <module>\n",
      "    r2: AIMessage(\n",
      "        content=(\n",
      "            \"Why don't bears wear socks?\\n\"\n",
      "            '\\n'\n",
      "            'Because they like to walk around with bear feet!'\n",
      "        ),\n",
      "        additional_kwargs={\n",
      "            'refusal': None,\n",
      "        },\n",
      "        response_metadata={\n",
      "            'token_usage': {\n",
      "                'completion_tokens': 17,\n",
      "                'prompt_tokens': 13,\n",
      "                'total_tokens': 30,\n",
      "                'completion_tokens_details': None,\n",
      "                'prompt_tokens_details': None,\n",
      "            },\n",
      "            'model_name': 'gpt-4',\n",
      "            'system_fingerprint': 'fp_5603ee5e2e',\n",
      "            'finish_reason': 'stop',\n",
      "            'logprobs': None,\n",
      "            'content_filter_results': {},\n",
      "        },\n",
      "        id='run-5ac88cce-be19-44ad-9929-b966f582e0e2-0',\n",
      "        usage_metadata={\n",
      "            'input_tokens': 13,\n",
      "            'output_tokens': 17,\n",
      "            'total_tokens': 30,\n",
      "            'input_token_details': {},\n",
      "            'output_token_details': {},\n",
      "        },\n",
      "    ) (AIMessage)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why don't bears wear socks?\\n\\nBecause they like to walk around with bear feet!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 13, 'total_tokens': 30, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4', 'system_fingerprint': 'fp_5603ee5e2e', 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-5ac88cce-be19-44ad-9929-b966f582e0e2-0', usage_metadata={'input_tokens': 13, 'output_tokens': 17, 'total_tokens': 30, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm_dyn = get_llm()\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "chain = prompt | llm_dyn\n",
    "#c1 = chain.with_config(configurable={\"llm\": \"llama3_70_groq\"})\n",
    "#r1 = c1.invoke({\"topic\": \"bears\"})\n",
    "#debug(r1)\n",
    "c2 = chain.with_config(configurable={\"llm\": \"gpt_35_openai\"})\n",
    "r2 = c2.invoke({\"topic\": \"bears\"})\n",
    "debug(r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
