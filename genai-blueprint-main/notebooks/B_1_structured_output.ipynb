{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from devtools import debug\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = '/mnt/c/Users/a884470/prj/genai-blueprint-main'  # Change this if needed\n",
    "sys.path.append(os.path.join(project_root, 'python'))\n",
    "load_dotenv(verbose=True)\n",
    "\n",
    "\n",
    "#!export PYTHONPATH=\"/mnt/c/Users/a884470/prj/genai-blueprint-main\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 : provide instruction in the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-13 17:53:32.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mconfig\u001b[0m:\u001b[36myaml_file_config\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mload /mnt/c/Users/a884470/prj/genai-blueprint-main/app_conf.yaml\u001b[0m\n",
      "\u001b[32m2024-11-13 17:53:32.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mconfig\u001b[0m:\u001b[36myaml_file_config\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mOverride config from env. variable: azure\u001b[0m\n",
      "\u001b[32m2024-11-13 17:53:32.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mai_core.llm\u001b[0m:\u001b[36mget_llm\u001b[0m:\u001b[36m409\u001b[0m - \u001b[1mget LLM:'gpt_4_azure' -json_mode\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30663/2095564118.py:30 <module>\n",
      "    r: Joke(\n",
      "        the_joke='Why was the cat sitting on the computer? Because it wanted to keep an eye on the mouse!',\n",
      "        explanation=(\n",
      "            \"This joke is a play on words. Cats are known for chasing mice, and 'mouse' also refers to the computer ac\"\n",
      "            \"cessory used to navigate the cursor on the screen. The humor comes from the double meaning of the word 'm\"\n",
      "            \"ouse' and the image of a cat literally watching a computer mouse.\"\n",
      "        ),\n",
      "        rate=3.5,\n",
      "    ) (Joke)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Joke(the_joke='Why was the cat sitting on the computer? Because it wanted to keep an eye on the mouse!', explanation=\"This joke is a play on words. Cats are known for chasing mice, and 'mouse' also refers to the computer accessory used to navigate the cursor on the screen. The humor comes from the double meaning of the word 'mouse' and the image of a cat literally watching a computer mouse.\", rate=3.5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The usual \"tell me a joke\" LLM call.\n",
    "\"\"\"\n",
    "\n",
    "from ai_core.llm import get_llm  # noqa: E402\n",
    "from ai_core.prompts import def_prompt # noqa: E402\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    the_joke: str = Field(description=\"a good joke\")\n",
    "    explanation: str = Field(description=\"explain why it's funny\")\n",
    "    rate: float = Field(description=\"rate how the joke is funny between 0 and 5\")\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt_with_format = \"\"\"\n",
    "    tell me  a joke on {topic}     \n",
    "    --- \n",
    "    {format_instructions}\"\"\"\n",
    "\n",
    "structured_prompt = def_prompt(user=prompt_with_format).partial(\n",
    "    format_instructions=parser.get_format_instructions(),\n",
    ")\n",
    "\n",
    "LLM_ID = None\n",
    "structured_joke = structured_prompt | get_llm(llm_id=LLM_ID, json_mode=True) | parser\n",
    "\n",
    "r = structured_joke.invoke({\"topic\": \"cat\"})\n",
    "debug(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30663/2639052080.py:1 <module>\n",
      "    structured_prompt: ChatPromptTemplate(\n",
      "        input_variables=['topic'],\n",
      "        input_types={},\n",
      "        partial_variables={\n",
      "            'format_instructions': (\n",
      "                'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n'\n",
      "                '\\n'\n",
      "                'As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strin'\n",
      "                'gs\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\n'\n",
      "                'the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"propertie'\n",
      "                's\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n'\n",
      "                '\\n'\n",
      "                'Here is the output schema:\\n'\n",
      "                '```\\n'\n",
      "                '{\"properties\": {\"the_joke\": {\"description\": \"a good joke\", \"title\": \"The Joke\", \"type\": \"string\"}, \"e'\n",
      "                'xplanation\": {\"description\": \"explain why it\\'s funny\", \"title\": \"Explanation\", \"type\": \"string\"}, \"ra'\n",
      "                'te\": {\"description\": \"rate how the joke is funny between 0 and 5\", \"title\": \"Rate\", \"type\": \"number\"}'\n",
      "                '}, \"required\": [\"the_joke\", \"explanation\", \"rate\"]}\\n'\n",
      "                '```'\n",
      "            ),\n",
      "        },\n",
      "        messages=[\n",
      "            HumanMessagePromptTemplate(\n",
      "                prompt=PromptTemplate(\n",
      "                    input_variables=[\n",
      "                        'format_instructions',\n",
      "                        'topic',\n",
      "                    ],\n",
      "                    input_types={},\n",
      "                    partial_variables={},\n",
      "                    template=(\n",
      "                        'tell me  a joke on {topic}     \\n'\n",
      "                        '--- \\n'\n",
      "                        '{format_instructions}'\n",
      "                    ),\n",
      "                ),\n",
      "                additional_kwargs={},\n",
      "            ),\n",
      "        ],\n",
      "    ) (ChatPromptTemplate) len=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['topic'], input_types={}, partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"the_joke\": {\"description\": \"a good joke\", \"title\": \"The Joke\", \"type\": \"string\"}, \"explanation\": {\"description\": \"explain why it\\'s funny\", \"title\": \"Explanation\", \"type\": \"string\"}, \"rate\": {\"description\": \"rate how the joke is funny between 0 and 5\", \"title\": \"Rate\", \"type\": \"number\"}}, \"required\": [\"the_joke\", \"explanation\", \"rate\"]}\\n```'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['format_instructions', 'topic'], input_types={}, partial_variables={}, template='tell me  a joke on {topic}     \\n--- \\n{format_instructions}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug(structured_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tell me  a joke on cat     \n",
      "--- \n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"the_joke\": {\"description\": \"a good joke\", \"title\": \"The Joke\", \"type\": \"string\"}, \"explanation\": {\"description\": \"explain why it's funny\", \"title\": \"Explanation\", \"type\": \"string\"}, \"rate\": {\"description\": \"rate how the joke is funny between 0 and 5\", \"title\": \"Rate\", \"type\": \"number\"}}, \"required\": [\"the_joke\", \"explanation\", \"rate\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# You can have a look at the generated prompt:\n",
    "print(structured_prompt.invoke({\"topic\": \"cat\"}).messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method #2 : Use \"with_structured_output\"  (bases on function calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-13 17:58:56.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mai_core.llm\u001b[0m:\u001b[36mget_llm\u001b[0m:\u001b[36m409\u001b[0m - \u001b[1mget LLM:'gpt_4_azure'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30663/925907821.py:6 <module>\n",
      "    chain.invoke(({\"topic\": \"cat\"})): Joke(\n",
      "        the_joke='Why was the cat sitting on the computer? Because it wanted to keep an eye on the mouse!',\n",
      "        explanation=(\n",
      "            \"This joke is a play on words. In the context of computers, a 'mouse' is a device used to navigate the cur\"\n",
      "            \"sor. However, in the context of cats, a 'mouse' is a small rodent that they often chase. The humor comes \"\n",
      "            \"from the double meaning of the word 'mouse' and the image of a cat literally sitting on a computer to wat\"\n",
      "            'ch a computer mouse, as if it were a real mouse.'\n",
      "        ),\n",
      "        rate=3.0,\n",
      "    ) (Joke)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Joke(the_joke='Why was the cat sitting on the computer? Because it wanted to keep an eye on the mouse!', explanation=\"This joke is a play on words. In the context of computers, a 'mouse' is a device used to navigate the cursor. However, in the context of cats, a 'mouse' is a small rodent that they often chase. The humor comes from the double meaning of the word 'mouse' and the image of a cat literally sitting on a computer to watch a computer mouse, as if it were a real mouse.\", rate=3.0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"tell me  a joke on {topic}\"\n",
    "\n",
    "# MODEL = None\n",
    "MODEL = \"gpt_4_azure\"\n",
    "chain = def_prompt(prompt) | get_llm(llm_id=MODEL).with_structured_output(Joke)\n",
    "debug(chain.invoke(({\"topic\": \"cat\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Assignement (Optional)\n",
    "Rate the above joke.\n",
    "Use https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/enum/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class JokeRater(Enum):\n",
    "    NOT_SO_GOOD = 0\n",
    "    GOOD = 1\n",
    "    VERY_GOOD = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-blueprint",
   "language": "python",
   "name": "genai-blueprint-ls3xo0xc-py3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
